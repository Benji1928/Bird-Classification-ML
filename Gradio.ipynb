{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524809cf-8ec4-4e6e-ba2a-d8f82e579740",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df399238-8190-4a29-9f17-898a610775d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import nntools\n",
    "from data import BirdDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6ac0df-0ffb-46e8-bb04-85c08153f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def criterion(self, y, d):\n",
    "        return self.cross_entropy(y, d)\n",
    "\n",
    "class VGG16(Classifier):\n",
    "    def __init__(self, num_classes, fine_tuning=False, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        vgg = tv.models.vgg16_bn(pretrained=True)\n",
    "        \n",
    "        # Apply freezing \n",
    "        for name, param in vgg.features.named_parameters():\n",
    "            layer_idx = int(name.split('.')[0])\n",
    "            # Freezing layers 0, 3, 7 (conv1_1, conv2_1, conv3_1)\n",
    "            if layer_idx in [0, 3, 7]:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.features = vgg.features\n",
    "\n",
    "        num_ftrs = 25088 \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.features(x).view(x.shape[0], -1) \n",
    "        y = self.classifier(f)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4207d376-cd50-4626-95b2-4cdad76d6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18Transfer(Classifier):\n",
    "    def __init__(self, num_classes, fine_tuning=False, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        resnet = tv.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Freeze or unfreeze layers based on fine_tuning flag\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = fine_tuning\n",
    "        \n",
    "        num_ftrs = resnet.fc.in_features\n",
    "        \n",
    "        # Replace the final fully connected layer with dropout + new classifier\n",
    "        resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.classifier = resnet\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f8d4ad-4961-4d23-a1e9-58ec42ac75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_CLASSES = 200 \n",
    "MODELS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a89570-b67e-4d13-a3e9-aef88c35ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name, model_class, model_dir):\n",
    "    model = model_class(NUM_CLASSES).to(device)\n",
    "    model_path = os.path.join(model_dir, \"checkpoint.pth.tar\")\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Determine the key for the state_dict based on nntools.Experiment saving format\n",
    "        if isinstance(checkpoint, dict) and 'Net' in checkpoint:\n",
    "             model.load_state_dict(checkpoint['Net'])\n",
    "        else:\n",
    "             # Fallback for directly saved state_dict\n",
    "             model.load_state_dict(checkpoint)\n",
    "             \n",
    "        model.eval()\n",
    "        print(f\"{name} model loaded successfully from {model_path}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {name} checkpoint not found at {model_path}. Skipping.\")\n",
    "        return None\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error loading {name} state dict: {e}. Skipping.\")\n",
    "        # If the checkpoint is the full Experiment object, you may need a deeper load:\n",
    "        # from nntools import Experiment \n",
    "        # exp = Experiment(model, None, None, None, None, output_dir=model_dir)\n",
    "        # exp.load_checkpoint(model_path)\n",
    "        # return exp.net.to(device)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486af5df-bb64-4295-b194-d64e469edb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Determine the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf8fd2c-7e58-4097-adbe-b343ee9ad388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benji\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\benji\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\benji\\AppData\\Local\\Temp\\ipykernel_3324\\3401142450.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 model loaded successfully from birdclass1\\checkpoint.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benji\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 model loaded successfully from birdclass2\\checkpoint.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Load VGG16 (birdclass1)\n",
    "VGG_MODEL_DIR = \"birdclass1\" \n",
    "MODELS['VGG16'] = load_model('VGG16', VGG16, VGG_MODEL_DIR)\n",
    "\n",
    "# Load ResNet18 (birdclass2)\n",
    "RESNET_MODEL_DIR = \"birdclass2\" \n",
    "MODELS['ResNet18'] = load_model('ResNet18', Resnet18Transfer, RESNET_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f4a1fd-a828-45f8-adc0-b8a508f397f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    tv.transforms.CenterCrop((224, 224)), # Center crop\n",
    "    tv.transforms.ToTensor(),           # Convert to tensor\n",
    "    tv.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]) # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435b3565-2a3b-4e06-916c-f6508f4c514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_names(num_classes):\n",
    "    # This should be replaced with your actual class name list\n",
    "    return [f\"Bird_Class_{i+1}\" for i in range(num_classes)]\n",
    "\n",
    "CLASS_NAMES = get_class_names(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8918d725-4c38-45ec-b35f-93aa198bd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(arch_name: str, img: Image.Image) -> dict:\n",
    "    \"\"\"\n",
    "    Predicts the class of the input image using the selected model.\n",
    "    \"\"\"\n",
    "    if img is None:\n",
    "        return {name: 0.0 for name in CLASS_NAMES}\n",
    "    \n",
    "    model = MODELS.get(arch_name)\n",
    "    if model is None:\n",
    "        return {\"Error\": f\"{arch_name} model is not loaded.\"}\n",
    "\n",
    "    # Apply preprocessing\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "\n",
    "    # Post-process: apply softmax\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    confidences = {CLASS_NAMES[i]: float(probabilities[i]) for i in range(NUM_CLASSES)}\n",
    "    \n",
    "    return confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f2d39e8-bcc6-4ecd-aa0a-9ecb95329707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model choices for the dropdown\n",
    "model_choices = list(MODELS.keys())\n",
    "# Filter out any models that failed to load\n",
    "model_choices = [name for name in model_choices if MODELS[name] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10007f1-7a5c-4ee9-b0f3-a9acfffb0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_choices:\n",
    "    raise RuntimeError(\"No models were loaded successfully. Check MODEL_PATH and file contents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5789483-13e2-4f48-8b2a-3c49642ac449",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropdown = gr.Dropdown(\n",
    "    label=\"Select Model Architecture\",\n",
    "    choices=model_choices,\n",
    "    value=model_choices[0], # Default to the first loaded model\n",
    "    interactive=True\n",
    ")\n",
    "\n",
    "image_input = gr.Image(\n",
    "    type=\"pil\", \n",
    "    label=\"Upload Bird Image (224x224 will be used)\", \n",
    "    width=224, \n",
    "    height=224\n",
    ")\n",
    "\n",
    "label_output = gr.Label(num_top_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa9477de-79b8-4202-8a16-16321499f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gradio Interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict_image,\n",
    "    inputs=[model_dropdown, image_input],\n",
    "    outputs=label_output,\n",
    "    title=\"Bird Species Classification: VGG16 vs. ResNet18\",\n",
    "    description=\"Select a pre-trained model and upload an image of a bird to get the top 5 predicted species.\",\n",
    "    examples=[] # Empty list for simplicity, but you can add valid paths here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b176b33b-8d2d-4877-80bb-be8e20a009c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

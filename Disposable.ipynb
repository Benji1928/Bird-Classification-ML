{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db314c-c536-45ae-9c57-e2446f054222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Transfer(NNClassifier):\n",
    "\n",
    "    def __init__(self, num_classes, fine_tuning=False):\n",
    "        super(VGG16Transfer, self).__init__()\n",
    "        vgg = tv.models.vgg16_bn(pretrained=True)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = fine_tuning\n",
    "        self.features = vgg.features\n",
    "        self.classifier = vgg.classifier\n",
    "        num_ftrs = vgg.classifier[6].in_features\n",
    "        self.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.features(x).view(x.shape[0], -1)       \n",
    "        y = self.classifier(f)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a319c4-9655-44fe-827e-2159e2da0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(Classifier):\n",
    "\n",
    "    def __init__(self, num_classes, fine_tuning=False):\n",
    "        super(VGG16, self).__init__()\n",
    "        \n",
    "        # Use the new weights argument instead of deprecated pretrained=True\n",
    "        vgg = tv.models.vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "        # Freeze or unfreeze all parameters depending on fine_tuning flag\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = fine_tuning\n",
    "\n",
    "        # Keep the convolutional feature extractor\n",
    "        self.features = vgg.features\n",
    "        \n",
    "        # Copy the classifier\n",
    "        self.classifier = vgg.classifier\n",
    "        \n",
    "        # Replace the final fully-connected layer to match number of classes\n",
    "        num_ftrs = vgg.classifier[6].in_features\n",
    "        self.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        f = self.features(x).view(x.shape[0], -1)\n",
    "        # Classify\n",
    "        y = self.classifier(f)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686272be-1d45-4373-af6f-c94b77395096",
   "metadata": {},
   "source": [
    "# Spare Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ebf52-e55b-4b1b-b51e-04cb83b12ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationStatsManager:\n",
    "    def __init__(self):\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        # Reset tracking variables\n",
    "        self.running_loss = 0.0\n",
    "        self.running_accuracy = 0.0\n",
    "        self.number_update = 0\n",
    "\n",
    "    def accumulate(self, loss, x, y, d):\n",
    "        # Accumulate total loss\n",
    "        self.running_loss += loss.item()\n",
    "\n",
    "        # Get predictions (highest logit per sample)\n",
    "        _, predicted = torch.max(y, dim=1)\n",
    "\n",
    "        # Compute accuracy for this batch\n",
    "        batch_accuracy = (predicted == d).float().mean().item()\n",
    "\n",
    "        # Accumulate totals\n",
    "        self.running_accuracy += batch_accuracy\n",
    "        self.number_update += 1\n",
    "\n",
    "    def summarize(self):\n",
    "        # Compute average loss and accuracy\n",
    "        avg_loss = self.running_loss / self.number_update\n",
    "        avg_acc = 100.0 * self.running_accuracy / self.number_update\n",
    "        return {'loss': avg_loss, 'accuracy': avg_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c9bdb-ef40-49a7-8e7a-57851cc7a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationStatsManager(nt.StatsManager):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClassificationStatsManager, self).__init__()\n",
    "\n",
    "    def init(self):\n",
    "        super(ClassificationStatsManager, self).init()\n",
    "        self.running_accuracy = 0\n",
    "\n",
    "    def accumulate(self, loss, x, y, d):\n",
    "        super(ClassificationStatsManager, self).accumulate(loss, x, y, d)\n",
    "        _, l = torch.max(y, 1)\n",
    "        self.running_accuracy += torch.mean((l == d).float())\n",
    "\n",
    "    def summarize(self):\n",
    "        loss = super(ClassificationStatsManager, self).summarize()\n",
    "        accuracy = 100 * self.running_accuracy / self.number_update\n",
    "        return {'loss': loss, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b6888-5b0d-45a1-aca7-300c2e85bd09",
   "metadata": {},
   "source": [
    "# Spare Function 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391daa1-2c41-46ea-93c1-c945c50f0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Setup ---\n",
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create model\n",
    "net = VGG16(num_classes)\n",
    "net = net.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Stats manager\n",
    "stats_manager = ClassificationStatsManager()\n",
    "\n",
    "# --- Training Loop (replaces nt.Experiment) ---\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    stats_manager.init()\n",
    "\n",
    "    for x, d in train_loader:\n",
    "        x, d = x.to(device), d.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y = net(x)\n",
    "        loss = criterion(y, d)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        stats_manager.accumulate(loss, x, y, d)\n",
    "\n",
    "    train_stats = stats_manager.summarize()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {train_stats['loss']:.4f}, Train Acc: {train_stats['accuracy']:.2f}%\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    net.eval()\n",
    "    stats_manager.init()\n",
    "    with torch.no_grad():\n",
    "        for x, d in val_loader:\n",
    "            x, d = x.to(device), d.to(device)\n",
    "            y = net(x)\n",
    "            loss = criterion(y, d)\n",
    "            stats_manager.accumulate(loss, x, y, d)\n",
    "\n",
    "    val_stats = stats_manager.summarize()\n",
    "    print(f\"           Val Loss: {val_stats['loss']:.4f}, Val Acc: {val_stats['accuracy']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9a04b-7b20-4245-ac77-26c68ecb1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "net = VGG16Transfer(num_classes)\n",
    "net = net.to(device)\n",
    "adam = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "stats_manager = ClassificationStatsManager()\n",
    "exp1 = nt.Experiment(net, train_set, val_set, adam, stats_manager,\n",
    "               output_dir=\"birdclass1\", perform_validation_during_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d5efc3-3276-4282-af2b-7515afbc8bda",
   "metadata": {},
   "source": [
    "# Spare Function 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d1f5e-85d6-49ff-a3bb-8439ab3bf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Training Dataset into Training and Val Datasets\n",
    "def split_train_val(df, target_column=\"class\", val_ratio=0.2, random_state=20, stratify=True):\n",
    "# random_state is the random seed for the splitting of training and val images\n",
    "# stratify preserve class distribution\n",
    "    \n",
    "    stratify_col = df[target_column] if stratify else None\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df['file_path'],\n",
    "        df[target_column],\n",
    "        test_size=val_ratio,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_col\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "def make_dataset_df(root_dir):\n",
    "data = []\n",
    "for cls in os.listdir(root_dir):\n",
    "    cls_path = os.path.join(root_dir, cls)\n",
    "    if os.path.isdir(cls_path):\n",
    "        for img_name in os.listdir(cls_path):\n",
    "            img_path = os.path.join(cls_path, img_name)\n",
    "            data.append({'file_path': img_path, 'class': cls})\n",
    "return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ace78-32c8-4596-8ac3-665b83920330",
   "metadata": {},
   "source": [
    "# Spare Function 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe53e4bb-1954-4490-b15d-6076f986704f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBirdDataset\u001b[39;00m(\u001b[43mtd\u001b[49m\u001b[38;5;241m.\u001b[39mDataset): \u001b[38;5;66;03m#td.Dataset is a class from td (torch.utils.data)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Constructor [Param: root file, train (training or testing dataset), download (online? if local x exist), target & label transform]\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Custom Dataset uses different Constructor [Param up to dev]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_dir, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, image_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m) ):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m() \u001b[38;5;66;03m# Call Parent __init__ function, ensure proper inheritance. (Newed Python)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'td' is not defined"
     ]
    }
   ],
   "source": [
    "class BirdDataset(td.Dataset): #td.Dataset is a class from td (torch.utils.data)\n",
    "\n",
    "    # Constructor [Param: root file, train (training or testing dataset), download (online? if local x exist), target & label transform]\n",
    "\n",
    "    # Custom Dataset uses different Constructor [Param up to dev]\n",
    "    def __init__(self, root_dir, mode=\"train\", image_size=(224,224) ):\n",
    "        super().__init__() # Call Parent __init__ function, ensure proper inheritance. (Newed Python)\n",
    "        self.image_size = image_size # Create new variable to store current self image_size \n",
    "        self.mode = mode\n",
    "        # Loading Dataset from Train Folder     \n",
    "        self.images_dir = os.path.join(dataset_root_dir, \"Train\")\n",
    "        ## Temp: Should Return \"E:\\Year 3 Sem 1\\COS30082 Applied Machine Learning\\Assignment_1\\Dataset\\Train\"\n",
    "        \n",
    "        # Loading .txt file (train.txt)\n",
    "        # Change to Local File Path\n",
    "        txt_path = os.path.join (root_dir, f'{mode}.txt')\n",
    "        print(\"Text File:\", txt_path)\n",
    "\n",
    "        \n",
    "        self.data = self.data = pd.read_csv(txt_path, sep=\" \", header=None, names=[\"file_path\", \"class\"])\n",
    "\n",
    "    \n",
    "    # Return No. of Data Images\n",
    "    def __len__(self): # __len__ is a method with built-in len() function\n",
    "        # print(\"Dataset Length:\", len(self.data))\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    # Return Init Configuration\n",
    "    def __repr__(self): \n",
    "        # __repr__ is a method to represent an object as a string. Images have x and y. Not using repr would result in the return of the memory location instead ( 0x10e104570)\n",
    "        # Can represent other things too from a memory location\n",
    "        \n",
    "        return \"BirdDataset: (mode='{}', image_size={})\".format(self.mode, self.image_size)\n",
    "\n",
    "    # Preparing the Images\n",
    "    def __getitem__(self,idx):\n",
    "        # Method automatically called, Allows class to behave like list or arrays\n",
    "        # Used to call and \"get\" datasets in the form of image and label\n",
    "        # Index 'idx'\n",
    "\n",
    "        # Image Path\n",
    "        img_path = os.path.join(self.images_dir, self.data.iloc[idx]['file_path'])\n",
    "        ## Temp: Should Return \"E:\\Year 3 Sem 1\\COS30082 Applied Machine Learning\\Assignment 1\\Dataset\\Train\" + \"Specific Image Location\"\n",
    "\n",
    "        # Load Image\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Transformation\n",
    "        transform = tv.transforms.Compose([\n",
    "            # Resize the Image\n",
    "            tv.transforms.Resize(self.image_size),\n",
    "\n",
    "            # Convert it to Tensor\n",
    "            tv.transforms.ToTensor(),\n",
    "\n",
    "            # Normalize it to the standard range (1, -1)\n",
    "            tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "            # C, H and W [Channel(Most common color channels), Height, Width]\n",
    "            # 1st tuple -> Mean for (R,G,B), 2nd tuple -> S.T.D for (R,G,B)\n",
    "        ])\n",
    "\n",
    "        # Processed Image    \n",
    "        x = transform(img)\n",
    "\n",
    "        # Get Class Label from Train.txt file\n",
    "        d = self.data.iloc[idx]['class']\n",
    "        \n",
    "        return x, d \n",
    "        \n",
    "    def number_of_classes(self):\n",
    "        return self.data['class'].max() + 1\n",
    "    # Max is used to find the largest class number (Classes are represented by numbers)\n",
    "    # Max + 1 because the index starts from 0 (To Compensate).\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1d718-8711-4efb-961b-c58a009c7a83",
   "metadata": {},
   "source": [
    "# Classifier (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bc439-c85b-447d-8d60-90683aa802f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(Classifier):\n",
    "\n",
    "    def __init__(self, num_classes, fine_tuning=False):\n",
    "        super(VGG16, self).__init__()\n",
    "        vgg = tv.models.vgg16_bn(pretrained=True)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = fine_tuning\n",
    "        self.features = vgg.features\n",
    "        self.classifier = vgg.classifier\n",
    "        num_ftrs = vgg.classifier[6].in_features\n",
    "        self.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.features(x).view(x.shape[0], -1)       \n",
    "        y = self.classifier(f)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8d75a-c34d-4135-ac71-d405122a87df",
   "metadata": {},
   "source": [
    "# Freezing (Mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337b618-0080-40ed-bee2-7682be22e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze first N layers\n",
    "for i, param in enumerate(self.features.parameters()):\n",
    "    if i < 20:  # freeze first 20 parameters (adjustable)\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9e151-a7d4-4ad0-8d34-46a82433bc63",
   "metadata": {},
   "source": [
    "# Weight Decay in Optimizer (Mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c07c31-7f2f-4147-9f49-a3a72b2a6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4  # L2 regularization\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520cec26-d67a-4296-bc9c-cc9c10f41372",
   "metadata": {},
   "source": [
    "# Early Stopping & LR Scheduling (Mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb216a-89a6-4a5a-969c-d9d97c36f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # training step ...\n",
    "    \n",
    "    val_loss = self.evaluate()[0]['loss']\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopper.step(val_loss)\n",
    "    if early_stopper.stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9522c6-38f7-41a9-b6eb-e62ce701435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(7, 3))\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    exp1.run(num_epochs=1, plot=lambda exp: plot(exp, fig=fig, axes=axes))\n",
    "    \n",
    "    val_loss = exp1.history[-1][1]['loss']  # validation loss\n",
    "    early_stopping.step(val_loss)\n",
    "    \n",
    "    if early_stopping.should_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319c4f0-023b-4f4b-ae7b-7c7405cf3889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDataset(td.Dataset):  # td.Dataset is torch.utils.data.Dataset\n",
    "    def __init__(self, root_dir, mode=\"train\", image_size=(224,224), transform=None):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "\n",
    "        # ✅ ADD THIS LINE (Fixes your AttributeError)\n",
    "        self.transform = transform   \n",
    "\n",
    "        # Loading Dataset from Train Folder\n",
    "        self.images_dir = os.path.join(root_dir, \"Train\")\n",
    "\n",
    "        # Load .txt file (train.txt / val.txt)\n",
    "        txt_path = os.path.join(root_dir, f'{mode}.txt')\n",
    "        print(\"Text File:\", txt_path)\n",
    "        self.data = pd.read_csv(txt_path, sep=\" \", header=None, names=[\"file_path\", \"class\"])\n",
    "\n",
    "        # ✅ REMOVE THESE if you plan to pass transforms externally\n",
    "        # self.train_transform = ...\n",
    "        # self.val_transform = ...\n",
    "\n",
    "        # ✅ OPTIONAL: add default transform (only used if none provided)\n",
    "        self.default_transform = tv.transforms.Compose([\n",
    "            tv.transforms.Resize(self.image_size),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"BirdDataset(mode='{self.mode}', image_size={self.image_size})\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.data.iloc[idx]['file_path'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = self.data.iloc[idx]['class']\n",
    "\n",
    "        # ✅ CHANGE THIS SECTION\n",
    "        # Use provided transform if available, else fallback to default\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = self.default_transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def number_of_classes(self):\n",
    "        return self.data['class'].max() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86665b24-02fb-4e97-b523-22141c81702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Resnet18Transfer(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True  # fine-tune all layers\n",
    "\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),  # reduce overfitting\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92588c5-f15a-49d7-a8b4-0c4b161605fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
